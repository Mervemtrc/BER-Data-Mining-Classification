# Auto detect text files and perform LF normalization
* text=auto
# -*- coding: utf-8 -*-
"""BER .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QG9ohXatgB46fJpKljIX04R6cRH2erm7
"""

#Importing all necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn import preprocessing

# Setting display max 500 rows and columns option
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)

# Reading the data using encoding format
df = pd.read_csv('BERPublicsearch.csv',encoding='latin1')

# Checking shape of the data (dataset contains more than a million rows and 211 columns)
df.shape

# Checking first 5 rows of the data
df.head()

"""# Selecting Data (not randomly but basis on fill rate and stratified sampling)

## Droping duplicate rows
"""

df1 = df.drop_duplicates()

"""## Checking data tpyes and missing values percentage"""

df1.info()

# creating dataframe of missing value percentage in dataset.
temp = pd.DataFrame(df1.isnull().sum()*100/len(df1)).reset_index().rename(columns={'index':'feature',0:'missing_per'})
temp

"""## Droping all columns where missing values are more than 70% since we have to select 200000 rows"""

# taking out of all feature names where the missing percentage is greater than or equal to 5%. (Thumb rule to handle missing values)
missing_greater5_cols = temp[temp['missing_per']>=5]['feature'].to_list()
missing_greater5_cols

# Dropping all the above cols where missing values are greater than 70%.
df1.drop(missing_greater5_cols,axis=1,inplace=True)

# Checking shape again (142 columns remaining)
df1.shape

# selecting all rows from dataframe where no column contains NaN.
df2 = df1[df1.notna().all(axis=1)]
df2.shape

# Checking the first 5 rows
df2.head()

"""### Now since there are 9,63,201 records available, we wil use stratified sampling technique to chose 2,00,000 rows out of it. we will not chose randomly. stratified sampling gives the proportionate presentation of all variables in the sample dataset.

## Stratified sampling based on all columns
"""

# to clear the memory we delete df
del df

# to clear the memory we delete df1
del df1

fraction = round(200000/len(df2),2)

# Perform stratified sampling (by taking n samples from each EnergyRating group to represent equal proportion of each combination)
sampled_df = df2.groupby('EnergyRating', group_keys=False).apply(lambda x: x.sample(frac=fraction))

sampled_df.shape

"""# Data Preprocessing

## missing values treatment if any
"""

# creating dataframe of missing value percentage in dataset.
temp1 = pd.DataFrame(sampled_df.isnull().sum()*100/len(sampled_df)).reset_index().rename(columns={'index':'feature',0:'missing_per'})
temp1

"""#### There are no missing values in feature columns.

## categorical to numerical conversion
"""

sampled_df.select_dtypes(include='object').head()

# Checking count of unique values for object data type features to check feasibility for converting them to numerical
for col in sampled_df.select_dtypes(include='object').columns:
    print("number of unique values in ",col," = ", sampled_df[col].nunique())

# Since DateOfAssessment and CountyName are having multiple unique values.
# So, it's not feasible to convert them into numerical values. Hence we drop these columns from dataset.
sampled_df = sampled_df.drop(['CountyName','DateOfAssessment'],axis=1)

# Convert EnergyRating feature to dependent variable as discussed in problem statement (If a BER of B or above indicates high energy efficiency(1) and a BER below B indicates low energy efficiency(0))
def high_energy_efficiency(df):
    if (df['EnergyRating'] in ('A1','A2','A3')):
        return 1
    else:
        return 0

sampled_df['high_energy_efficiency'] = sampled_df.apply(high_energy_efficiency, axis=1)

# dropping extra variable EnergyRating
sampled_df = sampled_df.drop('EnergyRating',axis=1)

# Getting dummy variables (converting categorical to numerical)
sampled_df = pd.get_dummies(sampled_df, drop_first=True)

sampled_df.shape

sampled_df.head()

# Double Checking if any missing value there.
sampled_df.isnull().sum()

"""#### So, finally our dataframe is ready with 2,02,272 records and 180 columns with zero missing values.

# Model Training
"""

# Droping all those features who are having correlation coefficient of greater than 0.8 or below -0.8. (positive and negative correlation)
corr_df = sampled_df.corr().abs()
mask = np.triu(np.ones_like(corr_df,dtype=bool))
tri_df = corr_df.mask(mask)
to_drop = [c for c in tri_df.columns if any(tri_df[c]>0.8) and c!='high_energy_efficiency']
to_drop

# Checking the distribution of y variable before modelling
sampled_df['high_energy_efficiency'].value_counts()

# Dropping those columns which are correlated.
sampled_df1 = sampled_df.drop(to_drop,axis=1)

# Defining X (independent) and Y (dependent) variables
X = sampled_df1.drop(['high_energy_efficiency'],axis=1)
Y = sampled_df1.high_energy_efficiency

#Train test split with a test set size of 20% of entire data
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=1)

#Standardizing the data
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""## Classification Model1 - Logistic Regression"""

#Logistic Regression
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

conf_matrix_lr = metrics.confusion_matrix(y_test, y_pred_lr)
print(conf_matrix_lr)

"""### Classification Model1 Evaluation"""

print('Accuracy = ',metrics.accuracy_score(y_test, y_pred_lr))
print('Error = ',1 - metrics.accuracy_score(y_test, y_pred_lr))
print('Precision = ',metrics.precision_score(y_test, y_pred_lr,))
print('Recall = ',metrics.recall_score(y_test, y_pred_lr))
print('F-1 Score = ',metrics.f1_score(y_test, y_pred_lr))
print('Classification Report\n',metrics.classification_report(y_test, y_pred_lr))

"""## Classification Model2 - Decision Tree Classifier"""

#Decision tree classifier
model = tree.DecisionTreeClassifier(criterion = "entropy", random_state = 42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

conf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(conf_matrix)

"""### Classification Model2 Evaluation"""

print('Accuracy = ',metrics.accuracy_score(y_test, y_pred))
print('Error = ',1 - metrics.accuracy_score(y_test, y_pred))
print('Precision = ',metrics.precision_score(y_test, y_pred,))
print('Recall = ',metrics.recall_score(y_test, y_pred))
print('F-1 Score = ',metrics.f1_score(y_test, y_pred))
print('Classification Report\n',metrics.classification_report(y_test, y_pred))

"""## Classification Model3 - Random Forest Classifier"""

#Random Forest classifier  - Best one
model = RandomForestClassifier(n_estimators = 10,criterion='entropy',random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

conf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(conf_matrix)

"""### Classification Model3 Evaluation"""

print('Accuracy = ',metrics.accuracy_score(y_test, y_pred))
print('Error = ',1 - metrics.accuracy_score(y_test, y_pred))
print('Precision = ',metrics.precision_score(y_test, y_pred,))
print('Recall = ',metrics.recall_score(y_test, y_pred))
print('F-1 Score = ',metrics.f1_score(y_test, y_pred))
print('Classification Report\n',metrics.classification_report(y_test, y_pred))

"""## Since the dataset is imbalanced (high energy records are 27074 which is 15.45%), So, recall should be our priority as an evaluation metric. Logistic Regression Model gives the best recall of 0.98. That means, out of 100 high energy cases, our model is rightly able to detect 98 out of them.

### Feature Importance
"""

temp = pd.DataFrame(lr.coef_[0],X.columns).reset_index().rename(columns={'index':'feature',0:'feature_importance'})
temp.sort_values('feature_importance', ascending=False)

"""## The most significant factors of high energy efficiency of a building are FirstFloorArea,
## GroundFloorArea, FirstEnerProdDelivered, MainWaterHeatingFuel_Heating , RoomInRoofArea.
"""

